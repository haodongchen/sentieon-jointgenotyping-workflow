{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c02645-f73e-4041-a63b-8970757e41d2",
   "metadata": {},
   "source": [
    "# Joint Cohort Calling\n",
    "For power users of CAVATICA with cohorts larger than 2,200, but also one in which any instance being used will not exceed 4TB in EBS storage\n",
    "1. Set up front parameters\n",
    "1. Set up CAVATICA credentials\n",
    "1. Set up split Split by chr jobs and run\n",
    "1. Tag split jobs so that they can be easily found and deleted later\n",
    "1. Run Genotyper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5fea6",
   "metadata": {},
   "source": [
    "## Set up initial parameters/variables\n",
    "To try and make this as reusable/flexible as possible, modify these variables to fit your specific project. _Run every time the kernel is restarted_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef9801",
   "metadata": {},
   "source": [
    "### Likely needs modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_name = \"default\" # Seven bridges authentication profile to use. See SBG docs https://docs.sevenbridges.com/docs/store-credentials-to-access-seven-bridges-client-applications-and-libraries if you haven't set this up\n",
    "project = 'd3b-bixu/kf-sentieon-join-cohort-genotyping-dev' # Project that contains your VCF files to joint call\n",
    "chr_list = 'split_vcf_chr_list.txt' # upload to your project a file with a list of chromosomes that you wish to joint call, new-line separated. Change this variable to what you named it.\n",
    "gvcf_tag = \"INPUT_GVCF_ONLY\" # It's recommended that you use the platform to add a tag to the GVCF files that you wish to joint call to find them more easily\n",
    "cohort_size  = 46 # Number of samples you are joint-calling\n",
    "split_app_name = project + \"/split-vcf-mini-wf\" # edit this based on what you named workflow/split_vcf_mini_wf.cwl when you pushed to project\n",
    "gvcf_out_file_prefix = \"CBTN-TEST_\" # Set to meet your personal preference for the joint call file names prefixes. Will be basically gvcf_out_file_prefix_{chr_shard}.vcf.gz\n",
    "gvcf_typer_app_name = project + \"/sentieon-gvcftyper\" # as above, set to name of app you pushed for tools/sentieon_gvcftyper.cwl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eae778",
   "metadata": {},
   "source": [
    "### Likely OK as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a248b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type=\"c5.12xlarge;ebs-gp2;4000\" # Tinker with this based on expected input size and level of desired stacking\n",
    "split_task_name_prefix =  \"Split GVCF by chr set: \" # edit if you'd rather name the tasks for GVCF split something else\n",
    "gvcf_typer_cpus =  48 # Set to match number of cores available in instance_type\n",
    "reference_name = \"Homo_sapiens_assembly38.fasta\" # Can obtain from Kids first\n",
    "dbsnp_name = \"Homo_sapiens_assembly38.dbsnp.vcf.gz\" # recommended, can be any common snps resource. If not desired, set to None\n",
    "sentieon_license=\"10.5.64.221:8990\"\n",
    "gvcf_typer_task_name_prefix = \"Sentieon GVCFtyper: \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d5d0ca-a9be-4f15-8b97-f8dc8c05119a",
   "metadata": {},
   "source": [
    "## Set up imports and creds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7ad6b",
   "metadata": {},
   "source": [
    "### Imports\n",
    "_Run every time the kernel is restarted_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff4a5c-d5e9-4dd3-bd54-e01cd9cfb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import sevenbridges as sbg\n",
    "from sevenbridges.errors import SbgError\n",
    "from sevenbridges.http.error_handlers import rate_limit_sleeper, maintenance_sleeper\n",
    "from getpass import getpass\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4ac38",
   "metadata": {},
   "source": [
    "### Set up Credentials Data Studio\n",
    "Use this if using a Data Studio Analysis on CAVATICA to set up the jobs.\n",
    "Credential set up using developer token allows you to set up task jobs. By design, token is deleted after creds file is created. Creds file should disappear after session which is secure. If you're running locally, adjust accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e580ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name =  \"/home/jovyan/.sevenbridges/credentials\"\n",
    "try:\n",
    "    os.mkdir(\"/home/jovyan/.sevenbridges\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "config_file=open(config_name, 'w')\n",
    "\n",
    "endpoint='api_endpoint = https://cavatica-api.sbgenomics.com/v2\\n'\n",
    "token = getpass('Enter your sbg token:')\n",
    "config_file.write(\"[default]\\n\" + endpoint + \"auth_token = \" + token)\n",
    "config_file.close()\n",
    "config = sbg.Config(profile='default')\n",
    "api = sbg.Api(config=config, error_handlers=[rate_limit_sleeper, maintenance_sleeper])\n",
    "del token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b40f1",
   "metadata": {},
   "source": [
    "### Set up Credentials Local\n",
    "Use this if running the notebook from a local workstation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = sbg.Config(profile=profile_name)\n",
    "api = sbg.Api(config=config, error_handlers=[rate_limit_sleeper, maintenance_sleeper])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec69a4-8714-47f6-9cf5-7348f798f617",
   "metadata": {},
   "source": [
    "## Set up split jobs\n",
    "Need to have created a list of chromosomes to use for splitting and downstream joint calling. Recommend chr1-22,X,Y. **Also recommend at project level to turn on Spot Instances and Memoization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb12c7-12c1-4dd7-8d47-6c0fc9afab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_obj = api.files.query(project=project, names=[chr_list])[0]\n",
    "chr_array = chr_obj.content().rstrip('\\n').split(\"\\n\")\n",
    "split_by_chr = True\n",
    "gvcf_files = api.files.query(project=project, tags=[gvcf_tag]).all()\n",
    "\n",
    "gvcf_set_list = []\n",
    "gvcf_set_list.append([])\n",
    "j=0\n",
    "x = 1\n",
    "# stack a min of 12 per instance, max out at 60 instances\n",
    "n = int(cohort_size/60)\n",
    "if n < 12:\n",
    "    n = 12\n",
    "\n",
    "# create set tasks - n per set\n",
    "for gvcf in gvcf_files:\n",
    "    if x > n:\n",
    "        gvcf_set_list.append([])\n",
    "        j += 1\n",
    "        x=1\n",
    "        print('Creating next set of ' + str(n), file=sys.stderr)\n",
    "    gvcf_set_list[j].append(gvcf)\n",
    "    x += 1\n",
    "ct = 1\n",
    "for gvcf_set in gvcf_set_list:\n",
    "    in_dict = {\"chr_list\": chr_obj, \"chr_array\": chr_array, \"input_vcf\": gvcf_set}\n",
    "    task_name = split_task_name_prefix + str(ct)\n",
    "    task = api.tasks.create(app=split_app_name, name=task_name, inputs=in_dict, project=project, run=False, execution_settings = {\"instance_type\": [instance_type]})\n",
    "    task.save()\n",
    "    ct +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b5898-d300-49e9-9e16-837e314e3f83",
   "metadata": {},
   "source": [
    "### Run the split tasks\n",
    "After reviewing/spot checking draft tasks to make sure they look ok, run them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218ee65-6b63-465d-847e-f79c718d3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = api.tasks.query(project=project, status=\"DRAFT\").all()\n",
    "for task in tasks:\n",
    "    if task.name.startswith(split_task_name_prefix):\n",
    "        task.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c274cbe-addd-4863-b2d1-b43ded564ddc",
   "metadata": {},
   "source": [
    "## Tag outputs\n",
    "**Only continue on after all Split tasks have completed!**\n",
    "\n",
    "Useful to organize inputs. Uses bulk get and bulk update - absolutely critical as per-file get and save will cause you to hit the api limit even if you have a turbo token real fast!\n",
    "If the metadata update fails for some reason, will go into debug mode to troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5525e63-f46b-4b04-9c6c-aad963664a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_obj = api.files.query(project=project, names=[chr_list])[0]\n",
    "chr_array = chr_obj.content().rstrip('\\n').split(\"\\n\")\n",
    "\n",
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "for task in tasks:\n",
    "    if task.name.startswith(split_task_name_prefix):\n",
    "        print(\"Tagging task \" + task.name)\n",
    "        for outdir in task.outputs['split_vcfs']:\n",
    "            split_set = api.files.bulk_get(outdir)\n",
    "            update_set = []\n",
    "            for bulk_file in split_set:\n",
    "                try:\n",
    "                    # update vcfs\n",
    "                    parts = bulk_file.resource.name.split(\"_\")\n",
    "                    bulk_file.resource.tags.extend([parts[0], \"INTERMEDIATE\"])\n",
    "                    update_set.append(bulk_file.resource)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"{} {}\".format(e, \"\\nFailed in VCF loop\"))\n",
    "                    pdb.set_trace()\n",
    "                    hold = 1\n",
    "            # update secondary files, just the intermediate tag to make it easy to find to delete\n",
    "            secondary = [x.secondary_files[0] for x in outdir]\n",
    "            bulk_index = api.files.bulk_get(secondary)\n",
    "            for index_file in bulk_index:\n",
    "                try:\n",
    "                    # update indexes\n",
    "                    index_file.resource.tags.append(\"INTERMEDIATE\")\n",
    "                    update_set.append(index_file.resource)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"{} {}\".format(e, \"\\nFailed in index loop\"))\n",
    "                    pdb.set_trace()\n",
    "                    hold = 1\n",
    "            try:\n",
    "                api.files.bulk_update(update_set)\n",
    "            except Exception as e:\n",
    "                print(\"{} {}\".format(e, \"\\nFailed in update\"))\n",
    "                pdb.set_trace()\n",
    "                hold=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba26833-6018-471c-a847-52ef97cb2652",
   "metadata": {},
   "source": [
    "## Set up joint call tasks\n",
    "Adjust the variables at the start as-needed to fit your project/preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adaa550-4973-459e-8295-448c5de4b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some static inputs here\n",
    "ref_dict={ \"reference\": api.files.query(project=project, names=[reference_name])[0], \"sentieon_license\": sentieon_license, \"cpu_per_job\": gvcf_typer_cpus }\n",
    "if dbsnp_name is not None:\n",
    "    ref_dict[\"dbSNP\"] =  api.files.query(project=project, names=[dbsnp_name])[0]\n",
    "\n",
    "for chrom in chr_array:\n",
    "    task_name = gvcf_typer_task_name_prefix + chrom\n",
    "    # convert collection object to list of objects, then sort to ensure each job has the samples in the same order\n",
    "    input_gvcf_collection = api.files.query(project=project, tags=[chrom]).all()\n",
    "    # list conversion might take 10 seconds or so\n",
    "    input_gvcf_list = list(input_gvcf_collection)\n",
    "    input_gvcf_list.sort(key=lambda x: x.name)\n",
    "    in_dict = {}\n",
    "    for key in ref_dict:\n",
    "        in_dict[key] = ref_dict[key]\n",
    "    in_dict['input_gvcf_files'] = input_gvcf_list\n",
    "    in_dict['interval'] = chrom\n",
    "    in_dict['output_file_name'] = gvcf_out_file_prefix + chrom + \".vcf.gz\"\n",
    "    task = api.tasks.create(app=gvcf_typer_app_name, name=task_name, inputs=in_dict, project=project, run=False, execution_settings = {\"instance_type\": [instance_type]} )\n",
    "    print(\"Created \" + task.name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f72b46-2c7a-475b-beba-9f217fe9089d",
   "metadata": {},
   "source": [
    "### Run the GT Tasks\n",
    "After reviewing/spot-checking drafted tasks, run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b20c0e-fe46-487b-97e2-429c0d303827",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = api.tasks.query(project=project, status=\"DRAFT\").all()\n",
    "for task in tasks:\n",
    "    if task.name.startswith(gvcf_typer_task_name_prefix):\n",
    "        task.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad958bc1",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "After all GT tasks are completed and you are satisfied with the results, it is highly reocmmended you delete all files tagged as `INTERMEDIATE`.\n",
    "You can do this in the CAVATICA GUI.\n",
    "Given the GUI limits, it may have to be done in batches of 40,000, as about that many will be selected when choosing \"select all\" that meet that criteria.\n",
    "**BE VERY CAREFUL WHEN DOING THIS. YOU CAN'T GET FILES BACK IF YOU ACCIDENTALLY DELETE THEM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53943e81-797d-4ad9-89e3-8b955dc9f3c7",
   "metadata": {},
   "source": [
    "## MISC\n",
    "Not needed but might be useful to get some cost and run time info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e588784-e3e2-441f-ad10-0caa94952327",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = api.tasks.query(project=project).all()\n",
    "phrase = split_task_name_prefix\n",
    "gt_phrase = gvcf_typer_task_name_prefix\n",
    "split_total = 0\n",
    "gt_total = 0\n",
    "gt_run_time = {}\n",
    "split_run_time = {}\n",
    "total_total = 0\n",
    "for task in tasks:\n",
    "    if task.price is not None:\n",
    "        run_hours=(task.end_time - task.start_time).seconds/3600\n",
    "        total_total += task.price.amount\n",
    "        if task.name.startswith(phrase):\n",
    "            split_total += task.price.amount\n",
    "            if task.name not in split_run_time:\n",
    "                split_run_time[task.name] = run_hours\n",
    "            else:\n",
    "                split_run_time[task.name] += run_hours\n",
    "        elif task.name.startswith(gt_phrase):\n",
    "            gt_total += task.price.amount\n",
    "            if task.name not in split_run_time:\n",
    "                gt_run_time[task.name] = run_hours\n",
    "            else:\n",
    "                gt_run_time[task.name] += run_hours\n",
    "\n",
    "print(\"Total spent so far: {}, Split costs: {}, GT costs {}\".format(total_total, split_total, gt_total))\n",
    "with open(\"split_run_times.txt\", 'w') as s:\n",
    "    for task_name in split_run_time:\n",
    "        print(\"{}\\t{}\".format(task_name, split_run_time[task_name]), file=s)\n",
    "with open(\"gt_run_times.txt\", 'w') as f:\n",
    "    for task_name in gt_run_time:\n",
    "        print(\"{}\\t{}\".format(task_name, gt_run_time[task_name]), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
